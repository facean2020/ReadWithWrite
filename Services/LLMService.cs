using System;
using System.Threading.Tasks;
using Microsoft.Extensions.Configuration;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Logging;

namespace ReadWithWrite.Services;

public interface ILLMService
{
    Task<string> GenerateResponseAsync(string systemPrompt, string userPrompt, bool enableSearch = false);
    Task<(string Revised, string Feedback)> ReviseAsync(string systemPrompt, string userContent);
}

public class LLMService : ILLMService
{
    private readonly ILLMProvider _provider;
    private readonly ILogger<LLMService> _logger;

    public LLMService(IServiceProvider serviceProvider, IConfiguration configuration, ILogger<LLMService> logger)
    {
        _logger = logger;

        // Production logic: Select Provider based on environment
        var apiKey = configuration["LLM:AliyunApiKey"];
        
        // Note: AliyunQwenProvider needs to be registered in Program.cs or instantiated here
        if (!string.IsNullOrEmpty(apiKey))
        {
            _logger.LogInformation("Aliyun API Key found. AliyunQwenProvider is available.");
            // _provider = serviceProvider.GetRequiredService<AliyunQwenProvider>();
        }
        else
        {
            _logger.LogWarning("Aliyun API Key is missing. Falling back to DefaultFallbackProvider.");
        }

        // Forced selection of Default Provider as per current requirement
        _logger.LogInformation("Forcing the use of DefaultFallbackProvider.");
        _provider = new DefaultFallbackProvider();
    }

    public async Task<string> GenerateResponseAsync(string systemPrompt, string userPrompt, bool enableSearch = false)
    {
        _logger.LogInformation("Generating response using {ProviderType}.", _provider.GetType().Name);
        return await _provider.ChatAsync(systemPrompt, userPrompt, enableSearch);
    }

    public async Task<(string Revised, string Feedback)> ReviseAsync(string systemPrompt, string userContent)
    {
        _logger.LogInformation("Starting revision process.");
        var response = await GenerateResponseAsync(systemPrompt, userContent);
        
        // Simple parsing logic, production may need more complex parsing (e.g., JSON or specific tags)
        // Currently handled simply
        return (response, "Feedback generated by LLM");
    }
}

public class MockLLMService : ILLMService
{
    public Task<string> GenerateResponseAsync(string systemPrompt, string userPrompt, bool enableSearch = false)
    {
        return Task.FromResult("Mock response");
    }

    public Task<(string Revised, string Feedback)> ReviseAsync(string systemPrompt, string userContent)
    {
        // Temporary mock implementation
        if (userContent.Contains("I has went"))
        {
            return Task.FromResult((
                "I went to school yesterday.",
                "Verb tense error: 'has went' â†’ 'went'. Also, 'I has' should be 'I have' or simply use past simple."
            ));
        }

        return Task.FromResult((
            userContent,
            "Your writing looks good! No major issues found."
        ));
    }
}
